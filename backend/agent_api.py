"""
Agent API - Phi√™n b·∫£n backend (kh√¥ng d√πng Streamlit).
File n√†y gom to√†n b·ªô c·∫•u tr√∫c agent LangChain, tool v√† c√°c h√†m wrapper
ƒë·ªÉ FastAPI c√≥ th·ªÉ g·ªçi tr·ª±c ti·∫øp. √ù t∆∞·ªüng t·ªïng qu√°t:

1. Kh·ªüi t·∫°o ToolCallingAgentRunner b·ªçc quanh ChatOpenAI v·ªõi kh·∫£ nƒÉng tool-calling.
2. ƒê·ªãnh nghƒ©a danh s√°ch tool (tr√≠ch xu·∫•t vƒÉn b·∫£n, l∆∞u session, ph√¢n t√≠ch k·ªπ nƒÉng, v.v.).
3. Cung c·∫•p c√°c h√†m API (analyze_cv_jd_api, chat_with_agent_api, ...) ƒë·ªÉ backend s·ª≠ d·ª•ng.

M·ªçi comment trong file ƒë·ªÅu c·ªë g·∫Øng gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc x·ª≠ l√Ω.
"""

import os  # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c hi·ªán t·∫°i v√† .env
import sys  # ƒêi·ªÅu ch·ªânh sys.path ƒë·ªÉ import n·ªôi b·ªô khi ch·∫°y d∆∞·ªõi d·∫°ng package
import base64  # M√£ h√≥a nh·ªã ph√¢n sang base64 (d√πng cho ·∫£nh/PDF)
import json  # Parse chu·ªói JSON t·ª´ ph·∫£n h·ªìi c·ªßa m√¥ h√¨nh
import re  # S·ª≠ d·ª•ng regex khi c·∫ßn
from typing import Any, Dict, List, Union  # Ki·ªÉu d·ªØ li·ªáu ch√∫ th√≠ch cho h√†m/method

# B·ªï sung parent directory v√†o sys.path ƒë·ªÉ import ƒë∆∞·ª£c modules khi ch·∫°y t·ª´ backend/.
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.tools import tool
from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env ·ªü project root (ph·ª•c v·ª• OpenAI key, Tavily...).
load_dotenv(os.path.join(os.path.dirname(current_dir), ".env"))

# --- C·∫•u h√¨nh Agent & Tool layer ---


class ToolCallingAgentRunner:
    """
    ƒê·ªëi t∆∞·ª£ng bao b·ªçc quanh ChatOpenAI ƒë·ªÉ:
    - Ti√™m system prompt.
    - Duy tr√¨ danh s√°ch tool v√† √°nh x·∫° t√™n -> callable.
    - L·∫∑p li√™n t·ª•c cho ƒë·∫øn khi m√¥ h√¨nh d·ª´ng vi·ªác g·ªçi tool v√† tr·∫£ output cu·ªëi.
    """

    def __init__(
        self,
        llm: ChatOpenAI,
        tools: List[Any],
        system_message: str = "",
        verbose: bool = False,
    ) -> None:
        self.llm = llm  # L∆∞u l·∫°i LLM g·ªëc (kh√¥ng r√†ng bu·ªôc tool) n·∫øu c·∫ßn t√°i s·ª≠ d·ª•ng.
        self.llm_with_tools = llm.bind_tools(tools)  # T·∫°o phi√™n b·∫£n LLM c√≥ kh·∫£ nƒÉng g·ªçi tool.
        self.tool_map = {tool.name: tool for tool in tools}  # T·∫°o map nhanh gi√∫p truy xu·∫•t tool theo t√™n.
        self.system_message = system_message  # L∆∞u system prompt ƒë·ªÉ lu√¥n g·ª≠i tr∆∞·ªõc user prompt.
        self.verbose = verbose  # C√≥ th·ªÉ b·∫≠t log debug (ch∆∞a d√πng hi·ªán t·∫°i).

    def _format_history(
        self, history: Union[List[BaseMessage], None, List[Any]]
    ) -> List[BaseMessage]:
        """Chu·∫©n h√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i th√†nh danh s√°ch LangChain message."""
        if not history:
            return []

        formatted: List[BaseMessage] = []  # Danh s√°ch k·∫øt qu·∫£ sau khi normalize.
        for item in history:
            if isinstance(item, BaseMessage):
                formatted.append(item)  # N·∫øu ƒë√£ l√† message c·ªßa LangChain th√¨ gi·ªØ nguy√™n.
                continue

            if isinstance(item, dict):
                role = item.get("role") or item.get("type")  # C√°c format custom c√≥ th·ªÉ d√πng 'role' ho·∫∑c 'type'.
                content = item.get("content", "")  # L·∫•y n·ªôi dung text.
                if role in ("human", "user"):
                    formatted.append(HumanMessage(content=str(content)))
                elif role in ("ai", "assistant"):
                    formatted.append(AIMessage(content=str(content)))
                elif role == "system":
                    formatted.append(SystemMessage(content=str(content)))
                elif role == "tool":
                    formatted.append(
                        ToolMessage(
                            content=str(content),
                            tool_call_id=item.get("tool_call_id", ""),
                        )
                    )
                continue

            if isinstance(item, str):
                formatted.append(HumanMessage(content=item))  # Chu·ªói thu·∫ßn ƒë∆∞·ª£c xem nh∆∞ l·ªùi ng∆∞·ªùi d√πng.

        return formatted

    def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        G·ª≠i prompt t·ªõi LLM, x·ª≠ l√Ω c√°c tool call tr·∫£ v·ªÅ v√† ti·∫øp t·ª•c cho ƒë·∫øn khi
        model ƒë∆∞a ra c√¢u tr·∫£ l·ªùi cu·ªëi c√πng.
        """
        user_input = inputs.get("input", "")  # Prompt ch√≠nh m√† caller cung c·∫•p.
        history = self._format_history(inputs.get("chat_history"))  # Chu·∫©n h√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.

        messages: List[BaseMessage] = []  # Danh s√°ch message g·ª≠i cho openai.
        if self.system_message:
            messages.append(SystemMessage(content=self.system_message))  # Th√™m system prompt n·∫øu c√≥.

        messages.extend(history or [])  # Th√™m c√°c message l·ªãch s·ª≠.
        messages.append(HumanMessage(content=user_input))  # Th√™m prompt hi·ªán t·∫°i.

        while True:
            response: AIMessage = self.llm_with_tools.invoke(messages)  # G·ªçi OpenAI (c√≥ kh·∫£ nƒÉng tool-calling).
            messages.append(response)  # L∆∞u l·∫°i ph·∫£n h·ªìi ƒë·ªÉ loop ti·∫øp (ghi nh·∫≠n tool_call, output, ...).

            tool_calls = getattr(response, "tool_calls", None) or []  # L·∫•y danh s√°ch tool_call t·ª´ ph·∫£n h·ªìi.
            if not tool_calls:
                return {"output": response.content, "messages": messages}  # N·∫øu kh√¥ng c√≥ tool_call -> k·∫øt th√∫c.

            for tool_call in tool_calls:
                tool_name = getattr(tool_call, "name", None) or getattr(
                    tool_call, "tool_name", None
                )
                tool_args = getattr(tool_call, "args", None) or getattr(
                    tool_call, "arguments", None
                )
                tool_call_id = getattr(tool_call, "id", None)

                if isinstance(tool_call, dict):
                    tool_name = tool_name or tool_call.get("name")
                    tool_args = tool_args or tool_call.get("args") or tool_call.get(
                        "arguments"
                    )
                    tool_call_id = tool_call_id or tool_call.get("id") or tool_call.get(
                        "tool_call_id"
                    )

                tool_instance = self.tool_map.get(tool_name)

                if not tool_instance:
                    tool_output = f"ERROR: Tool '{tool_name}' kh√¥ng t·ªìn t·∫°i."  # Sai t√™n tool -> th√¥ng b√°o l·ªói.
                else:
                    tool_params = tool_args or {}  # L·∫•y argument (c√≥ th·ªÉ l√† dict ho·∫∑c gi√° tr·ªã ƒë∆°n).
                    if not isinstance(tool_params, dict):
                        args_schema = getattr(tool_instance, "args_schema", None)
                        if args_schema and hasattr(args_schema, "__fields__"):
                            fields = list(args_schema.__fields__.keys())
                            if len(fields) == 1:
                                tool_params = {fields[0]: tool_params}
                            else:
                                tool_params = {}
                        else:
                            tool_params = {}

                    try:
                        tool_output = tool_instance.invoke(tool_params)  # Ch·∫°y tool th·ª±c t·∫ø.
                    except Exception as exc:
                        tool_output = f"ERROR: {exc}"

                if not isinstance(tool_output, str):
                    tool_output = str(tool_output)

                messages.append(
                    ToolMessage(content=tool_output, tool_call_id=tool_call_id or "")
                )  # ƒê∆∞a k·∫øt qu·∫£ tool v√†o history ƒë·ªÉ m√¥ h√¨nh ƒë·ªçc ƒë∆∞·ª£c.
            

# --- Import tool ph·ª• tr·ª£ (k√®m fallback khi ch·∫°y trong b·ªëi c·∫£nh kh√°c) ---
calculate_similarity = None  # Placeholder (ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ngay trong file n√†y).
process_raw_text = None  # S·∫Ω ƒë∆∞·ª£c g√°n sau khi import tools_ocr.

# C·ªë g·∫Øng ∆∞u ti√™n import module theo relative path (khi ch·∫°y nh∆∞ package).
try:
    from .tools_ocr import process_raw_text  # type: ignore
except ImportError:
    try:
        from tools_ocr import process_raw_text  # type: ignore
    except ImportError as e:
        print(f"‚ö†Ô∏è tools_ocr import error: {e}")  # Ghi log c·∫£nh b√°o n·∫øu kh√¥ng t√¨m th·∫•y module.

        def process_raw_text(text):
            return text.strip() if text else ""  # Fallback ƒë∆°n gi·∫£n: ch·ªâ strip kho·∫£ng tr·∫Øng hai ƒë·∫ßu.
    else:
        print("‚úÖ tools_ocr imported")  # Log khi import th√†nh c√¥ng ·ªü ki·ªÉu absolute.
else:
    print("‚úÖ tools_ocr imported (package relative)")  # Log khi import th√†nh c√¥ng ·ªü ki·ªÉu relative.

# Use OpenAI for similarity calculation
def calculate_similarity(cv_text, jd_text):
    """T√≠nh ƒëi·ªÉm ph√π h·ª£p CV-JD b·∫±ng GPT-4o"""
    try:
        llm = ChatOpenAI(model="gpt-4o", temperature=0)  # Kh·ªüi t·∫°o model nhi·ªát ƒë·ªô 0 ƒë·ªÉ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh.
        prompt = f"""B·∫°n l√† chuy√™n gia tuy·ªÉn d·ª•ng. H√£y ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p gi·ªØa CV v√† JD sau.

CV:
{cv_text[:3000]}

JD:
{jd_text[:2000]}

H√£y CH·ªà tr·∫£ v·ªÅ M·ªòT S·ªê t·ª´ 0.0 ƒë·∫øn 1.0 (v√≠ d·ª•: 0.75) th·ªÉ hi·ªán m·ª©c ƒë·ªô ph√π h·ª£p.
- 0.0-0.3: Kh√¥ng ph√π h·ª£p
- 0.3-0.5: √çt ph√π h·ª£p
- 0.5-0.7: Ph√π h·ª£p trung b√¨nh
- 0.7-0.85: Ph√π h·ª£p t·ªët
- 0.85-1.0: R·∫•t ph√π h·ª£p

CH·ªà TR·∫¢ V·ªÄ S·ªê, KH√îNG TH√äM G√å KH√ÅC."""
        
        response = llm.invoke([HumanMessage(content=prompt)])  # G·ª≠i prompt d∆∞·ªõi d·∫°ng HumanMessage.
        score_text = response.content.strip()  # L·∫•y chu·ªói k·∫øt qu·∫£ (ph·∫£i l√† s·ªë).
        
        # Parse score
        import re  # Import t·∫°i ch·ªó ƒë·ªÉ tr√°nh ph·ª• thu·ªôc global.
        match = re.search(r'(\d+\.?\d*)', score_text)  # T√¨m s·ªë d·∫°ng float trong chu·ªói.
        if match:
            score = float(match.group(1))
            return round(min(max(score, 0.0), 1.0), 4)
        return 0.5
    except Exception as e:
        print(f"Similarity error: {e}")
        return 0.5


# Global storage reference (ƒë∆∞·ª£c g√°n m·ªói request t·ª´ FastAPI).
_session_storage = {}  # FastAPI s·∫Ω truy·ªÅn dict session ƒë·ªÉ tools ƒë·ªçc v√† ghi.

def set_session_storage(storage):
    global _session_storage
    _session_storage = storage  # S·ª≠ d·ª•ng trong tr∆∞·ªùng h·ª£p c·∫ßn thay ƒë·ªïi storage runtime.

# ===== TOOLS =====
@tool
def tool_extract_text_from_file(file_path: str) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file (PDF ho·∫∑c ·∫£nh)."""
    try:
        ext = file_path.lower().split('.')[-1]  # L·∫•y ƒëu√¥i file ƒë·ªÉ quy·∫øt ƒë·ªãnh x·ª≠ l√Ω.
        
        # Handle PDF with PyMuPDF
        if ext == 'pdf':
            try:
                import fitz  # PyMuPDF
                doc = fitz.open(file_path)  # M·ªü file PDF.
                text_output = ""  # B·ªô ƒë·ªám l∆∞u text t·ªïng.
                for page in doc:
                    text_output += page.get_text() + "\n"  # L·∫•y text layer c·ªßa t·ª´ng trang.
                doc.close()
                
                if text_output.strip():
                    return text_output.strip()
                else:
                    return "PDF kh√¥ng c√≥ text layer."
            except ImportError:
                return "ERROR: PyMuPDF ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t."
            except Exception as e:
                return f"ERROR: Kh√¥ng th·ªÉ ƒë·ªçc PDF - {str(e)}"
        
        # Handle images with GPT-4o Vision
        else:
            with open(file_path, "rb") as f:
                file_bytes = f.read()  # ƒê·ªçc to√†n b·ªô bytes c·ªßa ·∫£nh.
                base64_data = base64.b64encode(file_bytes).decode('utf-8')  # Chuy·ªÉn sang base64 ƒë·ªÉ g·ª≠i cho GPT-4o.
            
            mime_type = f"image/{ext}" if ext != 'jpg' else "image/jpeg"
            
            vision_llm = ChatOpenAI(model="gpt-4o", temperature=0)  # D√πng GPT-4o Vision.
            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": "Tr√≠ch xu·∫•t TO√ÄN B·ªò vƒÉn b·∫£n trong h√¨nh ·∫£nh n√†y. Gi·ªØ nguy√™n format v√† c·∫•u tr√∫c. Ch·ªâ tr·∫£ v·ªÅ text, kh√¥ng th√™m gi·∫£i th√≠ch."
                    },
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime_type};base64,{base64_data}"}
                    }
                ]
            )
            response = vision_llm.invoke([message])
            return response.content
            
    except Exception as e:
        return f"ERROR: Kh√¥ng th·ªÉ ƒë·ªçc file - {str(e)}"  # Th√¥ng b√°o l·ªói chung n·∫øu c√≥ v·∫•n ƒë·ªÅ.


@tool
def tool_process_text_input(raw_text: str) -> str:
    """L√†m s·∫°ch vƒÉn b·∫£n."""
    try:
        return process_raw_text(raw_text)  # G·ªçi helper t·ª´ tools_ocr ƒë·ªÉ normalize text.
    except Exception as e:
        return f"ERROR: {str(e)}"


@tool
def tool_store_cv_text(cv_text: str) -> str:
    """L∆∞u CV text v√†o b·ªô nh·ªõ."""
    global _session_storage
    _session_storage["cv_text"] = cv_text  # Ghi l·∫°i ƒë·ªÉ c√°c tool kh√°c (jobs, chat, skills) s·ª≠ d·ª•ng.
    return f"SUCCESS: ƒê√£ l∆∞u CV text ({len(cv_text)} k√Ω t·ª±)"


@tool
def tool_store_jd_text(jd_text: str) -> str:
    """L∆∞u JD text v√†o b·ªô nh·ªõ."""
    global _session_storage
    _session_storage["jd_text"] = jd_text  # T∆∞∆°ng t·ª± cho JD.
    return f"SUCCESS: ƒê√£ l∆∞u JD text ({len(jd_text)} k√Ω t·ª±)"


@tool
def tool_calculate_match_score(dummy: str = "run") -> str:
    """T√≠nh ƒëi·ªÉm ph√π h·ª£p gi·ªØa CV v√† JD."""
    global _session_storage
    cv_text = _session_storage.get("cv_text", "")
    jd_text = _session_storage.get("jd_text", "")
    
    try:
        if not cv_text or not jd_text:
            return "ERROR: Ch∆∞a c√≥ CV ho·∫∑c JD text."
        score = calculate_similarity(cv_text, jd_text)  # G·ªçi helper GPT-4o ƒë·ªÉ t√≠nh ƒëi·ªÉm.
        return str(score)  # Tr·∫£ v·ªÅ chu·ªói ƒë·ªÉ agent d·ªÖ ch√®n v√†o b√°o c√°o.
    except Exception as e:
        return f"ERROR: {str(e)}"


@tool
def tool_find_jobs_online(search_query: str) -> str:
    """T√¨m ki·∫øm vi·ªác l√†m tr√™n m·∫°ng."""
    try:
        search_tool = TavilySearchResults(max_results=5)  # Kh·ªüi t·∫°o tool Tavily v·ªõi gi·ªõi h·∫°n 5 k·∫øt qu·∫£.
        results = search_tool.invoke({"query": search_query})  # Th·ª±c thi truy v·∫•n t√¨m ki·∫øm.
        
        formatted_results = ""  # Build chu·ªói markdown ƒë·ªÉ agent nh√∫ng v√†o b√°o c√°o.
        for item in results:
            formatted_results += f"- Ti√™u ƒë·ªÅ: {item.get('content', 'No content')[:100]}...\n"
            formatted_results += f"  Link: {item.get('url')}\n\n"
            
        return formatted_results
    except Exception as e:
        return f"ERROR searching jobs: {str(e)}"


@tool
def tool_analyze_skills(dummy: str = "run") -> str:
    """Ph√¢n t√≠ch k·ªπ nƒÉng trong CV so v·ªõi JD."""
    global _session_storage
    cv_text = _session_storage.get("cv_text", "")
    jd_text = _session_storage.get("jd_text", "")
    
    try:
        if not cv_text or not jd_text:
            return "ERROR: Ch∆∞a c√≥ CV ho·∫∑c JD text."

        llm = ChatOpenAI(model="gpt-4o", temperature=0)  # D√πng GPT-4o ƒë·ªÉ suy lu·∫≠n k·ªπ nƒÉng.
        prompt = (
            "B·∫°n l√† chuy√™n gia tuy·ªÉn d·ª•ng. H√£y ph√¢n t√≠ch CV c·ªßa ·ª©ng vi√™n so v·ªõi m√¥ t·∫£ "
            "c√¥ng vi·ªác (JD) v√† suy lu·∫≠n c√°c nh√≥m k·ªπ nƒÉng quan tr·ªçng.\n"
            "Tr·∫£ v·ªÅ JSON v·ªõi 4 m·∫£ng: cv_skills, jd_skills, matched_skills, missing_skills. "
            "M·ªói m·∫£ng li·ªát k√™ t·ªëi ƒëa 20 k·ªπ nƒÉng d·∫°ng c·ª•m ng·∫Øn.\n"
            "Quy t·∫Øc:\n"
            "- cv_skills: k·ªπ nƒÉng ·ª©ng vi√™n th·ªÉ hi·ªán r√µ trong CV.\n"
            "- jd_skills: k·ªπ nƒÉng/ƒëi·ªÅu ki·ªán c·ªët l√µi JD y√™u c·∫ßu.\n"
            "- matched_skills: giao gi·ªØa hai danh s√°ch (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng).\n"
            "- missing_skills: k·ªπ nƒÉng JD y√™u c·∫ßu nh∆∞ng CV ch∆∞a ch·ª©ng minh.\n"
            "- D√πng ƒë·ªãnh d·∫°ng ch·ªØ Title Case, tr√°nh tr√πng l·∫∑p.\n"
            "- CH·ªà tr·∫£ JSON, kh√¥ng th√™m m√¥ t·∫£ ho·∫∑c markdown.\n\n"
            f"CV TEXT:\n{cv_text[:6000]}\n\n"
            f"JOB DESCRIPTION:\n{jd_text[:6000]}"
        )

        response = llm.invoke([HumanMessage(content=prompt)])  # Prompt d∆∞·ªõi d·∫°ng HumanMessage.
        content = response.content.strip()  # Chu·∫©n h√≥a chu·ªói tr·∫£ v·ªÅ.

        # M·ªôt s·ªë model c√≥ th·ªÉ tr·∫£ JSON n·∫±m trong code block, t√°ch ra n·∫øu c·∫ßn.
        if content.startswith("```"):
            content = content.strip("`")
            if "\n" in content:
                content = content.split("\n", 1)[1]

        try:
            parsed = json.loads(content)  # C·ªë g·∫Øng parse JSON nguy√™n v·∫πn.
        except json.JSONDecodeError:
            parsed = None

        if isinstance(parsed, dict):
            cv_skills = ", ".join(parsed.get("cv_skills", []))
            jd_skills = ", ".join(parsed.get("jd_skills", []))
            matched_skills = ", ".join(parsed.get("matched_skills", []))
            missing_skills = ", ".join(parsed.get("missing_skills", []))
            return (
                f"cv_skills: {cv_skills} ||| "
                f"jd_skills: {jd_skills} ||| "
                f"matched_skills: {matched_skills} ||| "
                f"missing_skills: {missing_skills}"
            )

        # N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, tr·∫£ v·ªÅ raw content ƒë·ªÉ agent t·ª± x·ª≠ l√Ω.
        return content
    except Exception as e:
        return f"ERROR: {str(e)}"


@tool
def tool_suggest_jobs(dummy: str = "run") -> str:
    """G·ª£i √Ω vi·ªác l√†m ph√π h·ª£p."""
    global _session_storage
    cv_text = _session_storage.get("cv_text", "")  # Ch·ªâ c·∫ßn CV ƒë·ªÉ g·ªçi Tavily.
    
    if not cv_text:
        return "ERROR: Ch∆∞a c√≥ CV."
    
    return f"CV_CONTENT_FOR_ANALYSIS:\n{cv_text[:2000]}"


@tool
def tool_suggest_cv_improvements(dummy: str = "run") -> str:
    """ƒê·ªÅ xu·∫•t ch·ªânh s·ª≠a CV."""
    global _session_storage
    cv_text = _session_storage.get("cv_text", "")
    jd_text = _session_storage.get("jd_text", "")
    
    if not cv_text:
        return "ERROR: Ch∆∞a c√≥ CV."
    
    vision_llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
    jd_context = f"\n\nJD M·ª§C TI√äU:\n{jd_text[:2000]}" if jd_text else ""
    
    prompt = f"""B·∫°n l√† chuy√™n gia t∆∞ v·∫•n CV. H√£y ph√¢n t√≠ch CV sau v√† ƒê·ªÄ XU·∫§T B·∫¢N CV M·ªöI ƒê√É ƒê∆Ø·ª¢C CH·ªàNH S·ª¨A.

CV HI·ªÜN T·∫†I:
{cv_text[:3500]}
{jd_context}

OUTPUT FORMAT:
## üìã PH√ÇN T√çCH CV HI·ªÜN T·∫†I
[ƒêi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu]

## ‚úèÔ∏è ƒê·ªÄ XU·∫§T CH·ªàNH S·ª¨A
[Li·ªát k√™ c·ª• th·ªÉ nh·ªØng thay ƒë·ªïi]

## üìÑ CV M·ªöI ƒê√É CH·ªàNH S·ª¨A
```
[N·ªôi dung CV ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i ho√†n ch·ªânh]
```

## üí° GHI CH√ö QUAN TR·ªåNG
[L·ªùi khuy√™n th√™m]
"""
    
    try:
        response = vision_llm.invoke([HumanMessage(content=prompt)])
        return response.content  # Tr·∫£ nguy√™n vƒÉn ƒë·ªÉ frontend hi·ªÉn th·ªã markdown.
    except Exception as e:
        return f"ERROR: {str(e)}"


@tool  
def tool_analyze_cv_layout(file_path: str) -> str:
    """Ph√¢n t√≠ch layout CV t·ª´ file ·∫£nh."""
    try:
        with open(file_path, "rb") as f:
            file_bytes = f.read()  # ƒê·ªçc nh·ªã ph√¢n file ƒë√£ upload.
            base64_data = base64.b64encode(file_bytes).decode('utf-8')  # Encode base64 cho GPT-4o.
        
        ext = file_path.lower().split('.')[-1]
        if ext == 'pdf':
            mime_type = "application/pdf"
        else:
            mime_type = f"image/{ext}" if ext != 'jpg' else "image/jpeg"
        
        vision_llm = ChatOpenAI(model="gpt-4o", temperature=0)  # Vision mode ƒë√°nh gi√° layout.
        
        analysis_prompt = """B·∫°n l√† chuy√™n gia ƒë√°nh gi√° CV. H√£y PH√ÇN T√çCH CHI TI·∫æT LAYOUT/B·ªê C·ª§C c·ªßa CV n√†y.

TI√äU CH√ç ƒê√ÅNH GI√Å (1-10 ƒëi·ªÉm):
1. üìê B·ªê C·ª§C T·ªîNG TH·ªÇ
2. üî§ TYPOGRAPHY
3. üé® THI·∫æT K·∫æ & M√ÄU S·∫ÆC
4. üìã C·∫§U TR√öC SECTIONS
5. üìä T√çNH CHUY√äN NGHI·ªÜP

OUTPUT FORMAT:
## üéØ ƒê√ÅNH GI√Å LAYOUT CV
### ƒêi·ªÉm T·ªïng: X/10
| Ti√™u Ch√≠ | ƒêi·ªÉm | Nh·∫≠n X√©t |
|----------|------|----------|
| ... | X/10 | ... |

### ‚úÖ ƒêI·ªÇM T·ªêT
### ‚ö†Ô∏è C·∫¶N C·∫¢I THI·ªÜN
### üí° ƒê·ªÄ XU·∫§T CH·ªàNH S·ª¨A LAYOUT
"""
        
        message = HumanMessage(
            content=[
                {"type": "text", "text": analysis_prompt},
                {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_data}"}}
            ]
        )
        
        response = vision_llm.invoke([message])
        return response.content
        
    except Exception as e:
        return f"ERROR: {str(e)}"  # Tr·∫£ l·ªói ƒë·ªÉ agent hi·ªÉn th·ªã cho ng∆∞·ªùi d√πng.


@tool
def tool_generate_improved_cv_image(dummy: str = "run") -> str:
    """T·∫°o m√¥ t·∫£ layout CV m·ªõi."""
    global _session_storage
    cv_text = _session_storage.get("cv_text", "")  # D·ª±a v√†o n·ªôi dung CV hi·ªán t·∫°i.
    
    if not cv_text:
        return "ERROR: Ch∆∞a c√≥ CV."
    
    vision_llm = ChatOpenAI(model="gpt-4o", temperature=0.3)  # Nhi·ªát ƒë·ªô cao h∆°n ƒë·ªÉ ƒëa d·∫°ng √Ω t∆∞·ªüng layout.
    
    prompt = f"""D·ª±a tr√™n n·ªôi dung CV b√™n d∆∞·ªõi, h√£y t·∫°o M√î T·∫¢ CHI TI·∫æT v·ªÅ m·ªôt b·∫£n CV m·ªõi v·ªõi LAYOUT CHUY√äN NGHI·ªÜP.

N·ªòI DUNG CV:
{cv_text[:3000]}

T·∫†O M√î T·∫¢ VISUAL LAYOUT M·ªöI:
## üñºÔ∏è M√î T·∫¢ LAYOUT CV M·ªöI
### C·∫•u tr√∫c t·ªïng th·ªÉ
### Header Section
### Main Sections
### Color Scheme
### Typography Guide
### Visual Elements
## üìù N·ªòI DUNG CV ƒê√É T·ªêI ∆ØU
## üîó TEMPLATE G·ª¢I √ù
"""
    
    try:
        response = vision_llm.invoke([HumanMessage(content=prompt)])
        return response.content  # K·∫øt qu·∫£ l√† ƒëo·∫°n m√¥ t·∫£ chi ti·∫øt layout m·ªõi.
    except Exception as e:
        return f"ERROR: {str(e)}"


def initialize_agent_api(verbose: bool = False) -> ToolCallingAgentRunner:
    """Kh·ªüi t·∫°o agent v·ªõi b·ªô tool ti√™u chu·∫©n d√πng chung cho m·ªçi t√°c v·ª•."""
    llm = ChatOpenAI(model="gpt-4o", temperature=0)  # M·∫∑c ƒë·ªãnh d√πng GPT-4o v√† nhi·ªát ƒë·ªô 0.

    tools = [
        tool_extract_text_from_file,
        tool_process_text_input,
        tool_store_cv_text,
        tool_store_jd_text,
        tool_calculate_match_score,
        tool_analyze_skills,
        tool_suggest_jobs,
        tool_find_jobs_online,
        tool_suggest_cv_improvements,
        tool_analyze_cv_layout,
        tool_generate_improved_cv_image,
    ]

    system_message = (
        "B·∫°n l√† AI Recruitment Expert chuy√™n nghi·ªáp.\n\n"
        "NHI·ªÜM V·ª§:\n"
        "- Ph√¢n t√≠ch CV/JD, t√≠nh ƒëi·ªÉm, so s√°nh k·ªπ nƒÉng.\n"
        "- G·ª£i √Ω vi·ªác l√†m v√† ƒë√°nh gi√° tr·∫°ng th√°i ph·ªèng v·∫•n.\n"
        "- ƒê·ªÅ xu·∫•t ch·ªânh s·ª≠a CV b·∫±ng ti·∫øng Anh.\n"
        "- Ph√¢n t√≠ch layout CV khi ƒë∆∞·ª£c y√™u c·∫ßu.\n\n"
        "QUAN TR·ªåNG:\n"
        "- V·ªõi file: D√πng tool_extract_text_from_file.\n"
        "- Lu√¥n l∆∞u CV/JD sau khi tr√≠ch xu·∫•t.\n"
        "- Tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ ƒë·ªçc."
    )

    return ToolCallingAgentRunner(
        llm=llm,
        tools=tools,
        system_message=system_message,
        verbose=verbose,
    )


# ===== API FUNCTIONS =====
# C√°c h√†m d∆∞·ªõi ƒë√¢y ƒë∆∞·ª£c FastAPI g·ªçi tr·ª±c ti·∫øp.

def analyze_cv_jd_api(cv_input: str, jd_input: str, cv_type: str, jd_type: str, storage: dict) -> str:
    """API version of analyze_cv_jd"""
    global _session_storage
    _session_storage = storage  # Cho ph√©p tool layer truy c·∫≠p c√πng session dict.
    
    agent = initialize_agent_api()  # M·ªói request t·∫°o agent m·ªõi ƒë·ªÉ tr√°nh r√≤ r·ªâ state.
    
    user_query = f"""
Th·ª±c hi·ªán ph√¢n t√≠ch CV-JD:

TH√îNG TIN:
- CV: type={cv_type}, data={cv_input[:150]}...
- JD: type={jd_type}, data={jd_input[:150]}...

B∆Ø·ªöC 1: TR√çCH XU·∫§T CV TEXT
N·∫øu cv_type == 'file': G·ªçi tool_extract_text_from_file("{cv_input}")
N·∫øu cv_type == 'text': G·ªçi tool_process_text_input v·ªõi n·ªôi dung CV
SAU ƒê√ì: G·ªçi tool_store_cv_text v·ªõi k·∫øt qu·∫£

B∆Ø·ªöC 2: TR√çCH XU·∫§T JD TEXT
L√†m t∆∞∆°ng t·ª± v·ªõi JD, SAU ƒê√ì: G·ªçi tool_store_jd_text

B∆Ø·ªöC 3: T√çNH ƒêI·ªÇM PH√ô H·ª¢P
G·ªçi: tool_calculate_match_score("run")

B∆Ø·ªöC 4: PH√ÇN T√çCH K·ª∏ NƒÇNG
G·ªçi: tool_analyze_skills("run")

B∆Ø·ªöC 5: G·ª¢I √ù KH√ìA H·ªåC
D·ª±a v√†o missing_skills, ƒë·ªÅ xu·∫•t 3-5 kh√≥a h·ªçc t·ª´ Coursera, Udemy, edX.

B∆Ø·ªöC 6: VI·∫æT B√ÅO C√ÅO
# üìä K·∫æT QU·∫¢ PH√ÇN T√çCH
## üéØ ƒêi·ªÉm Ph√π H·ª£p: [SCORE]
## ‚úÖ K·ªπ NƒÉng ƒê√£ C√≥
## ‚ö†Ô∏è K·ªπ NƒÉng C·∫ßn B·ªï Sung
## üìö Kh√≥a H·ªçc ƒê·ªÅ Xu·∫•t
## üí° Nh·∫≠n X√©t
"""
    
    try:
        result = agent.invoke({"input": user_query, "chat_history": []})  # G·ª≠i prompt cho agent.
        return result['output']  # L·∫•y ph·∫ßn output cu·ªëi.
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"


def find_suitable_jobs_api(storage: dict) -> str:
    """API version of find_suitable_jobs"""
    global _session_storage
    _session_storage = storage  # Cho ph√©p tool layer ƒë·ªçc CV/JD ƒë√£ l∆∞u.
    
    cv_content = storage.get("cv_text", "")
    jd_content = storage.get("jd_text", "")
    
    if not cv_content:
        return "‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu CV. Vui l√≤ng ph√¢n t√≠ch CV tr∆∞·ªõc!"
    
    agent = initialize_agent_api()
    
    jd_context = f"\nJD ƒê√É PH√ÇN T√çCH:\n{jd_content[:2000]}" if jd_content else ""
    
    query = f"""
D·ª±a v√†o CV b√™n d∆∞·ªõi, th·ª±c hi·ªán:
1. Ph√¢n t√≠ch h·ªì s∆°
2. S·ª≠ d·ª•ng tool_find_jobs_online ƒë·ªÉ t√¨m 5+ c√¥ng vi·ªác ƒëang tuy·ªÉn
3. ƒê√°nh gi√° TR·∫†NG TH√ÅI PH·ªéNG V·∫§N cho m·ªói v·ªã tr√≠

N·ªòI DUNG CV:
{cv_content[:4000]}
{jd_context}

Y√äU C·∫¶U OUTPUT:
# üíº G·ª¢I √ù VI·ªÜC L√ÄM

## üîç Ph√¢n T√≠ch Nhanh
## üéØ ƒê√ÅNH GI√Å KH·∫¢ NƒÇNG PH·ªéNG V·∫§N
- **Kh·∫£ nƒÉng ƒë∆∞·ª£c g·ªçi ph·ªèng v·∫•n:** [Cao/Trung b√¨nh/Th·∫•p]
- **ƒêi·ªÉm m·∫°nh khi ph·ªèng v·∫•n:**
- **C·∫ßn chu·∫©n b·ªã th√™m:**

## üåê C√°c C√¥ng Vi·ªác ƒêang Tuy·ªÉn
### 1. [T√™n V·ªã Tr√≠] - [C√¥ng Ty]
   - üîó **Link:**
   - üìä **M·ª©c ƒë·ªô ph√π h·ª£p:**
   - üìû **Kh·∫£ nƒÉng tr√∫ng tuy·ªÉn:** üü¢/üü°/üî¥

## üí° L·ªúI KHUY√äN CHU·∫®N B·ªä PH·ªéNG V·∫§N
"""
    
    try:
        result = agent.invoke({"input": query, "chat_history": []})
        return result['output']
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"


def chat_with_agent_api(user_message: str, storage: dict) -> str:
    """API version of chat_with_agent"""
    global _session_storage
    _session_storage = storage  # ƒê·ªìng b·ªô session ƒë·ªÉ tool ƒë·ªçc th√¥ng tin CV/JD.
    
    agent = initialize_agent_api()
    
    cv_text = storage.get("cv_text", "")
    jd_text = storage.get("jd_text", "")
    chat_history = storage.get("chat_history", [])

    context_data = ""
    if cv_text:
        context_data += f"\n=== N·ªòI DUNG CV ===\n{cv_text[:3000]}\n"
    if jd_text:
        context_data += f"\n=== N·ªòI DUNG JD ===\n{jd_text[:3000]}\n"

    history_text = "\n".join(chat_history[-6:])

    full_query = f"""
TH√îNG TIN NG·ªÆ C·∫¢NH:
{context_data}

L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
{history_text}

C√ÇU H·ªéI M·ªöI:
{user_message}
"""

    try:
        result = agent.invoke({"input": full_query, "chat_history": []})
        output_text = result['output']
        
        # Save to history
        storage["chat_history"].append(f"User: {user_message}")
        storage["chat_history"].append(f"AI: {output_text}")
        
        return output_text
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"


def suggest_cv_improvements_api(storage: dict) -> dict:
    """API version of suggest_cv_improvements"""
    global _session_storage
    _session_storage = storage  # ƒê·ªìng b·ªô session cho layer tool.
    
    if not storage.get("cv_text"):
        return {"success": False, "output": "‚ùå Ch∆∞a c√≥ CV. Vui l√≤ng ph√¢n t√≠ch CV tr∆∞·ªõc!"}
    
    agent = initialize_agent_api()
    
    try:
        result = agent.invoke(
            {
                "input": (
                    "Please call tool_suggest_cv_improvements and deliver the rewritten CV entirely in English. "
                    "Do not include Vietnamese explanations."
                ),
                "chat_history": [],
            }
        )
        output_text = result["output"]
        response_payload = {"success": True, "output": output_text}

        return response_payload
    except Exception as e:
        return {"success": False, "output": f"‚ùå L·ªói: {str(e)}"}


def analyze_cv_layout_api(file_path: str) -> str:
    """API version of analyze_cv_layout"""
    agent = initialize_agent_api()
    
    try:
        result = agent.invoke({
            "input": f"H√£y s·ª≠ d·ª•ng tool_analyze_cv_layout v·ªõi file '{file_path}' ƒë·ªÉ ph√¢n t√≠ch layout CV.",
            "chat_history": []
        })
        return result['output']
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"


def generate_improved_cv_api(storage: dict) -> str:
    """API version of generate_improved_cv"""
    global _session_storage
    _session_storage = storage  # ƒê·∫£m b·∫£o tool s·ª≠ d·ª•ng chung session dict.
    
    if not storage.get("cv_text"):
        return "‚ùå Ch∆∞a c√≥ CV. Vui l√≤ng ph√¢n t√≠ch CV tr∆∞·ªõc!"
    
    agent = initialize_agent_api()
    
    try:
        result = agent.invoke({
            "input": "H√£y s·ª≠ d·ª•ng tool_generate_improved_cv_image ƒë·ªÉ t·∫°o m√¥ t·∫£ layout CV m·ªõi.",
            "chat_history": []
        })
        return result['output']
    except Exception as e:
        return f"‚ùå L·ªói: {str(e)}"

