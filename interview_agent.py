"""
Interview Agent - Ph·ªèng V·∫•n ·∫¢o v·ªõi AI
=====================================
- T·∫°o c√¢u h·ªèi ph·ªèng v·∫•n d·ª±a tr√™n CV v√† JD
- Ph√¢n t√≠ch video tr·∫£ l·ªùi (khu√¥n m·∫∑t, gi·ªçng n√≥i)
- ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi
- ƒê√°nh gi√° behavioral/communication skills
"""

import os
import sys
import base64
import json
import tempfile
from typing import Optional, List, Dict
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv

# Add current directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

load_dotenv(os.path.join(current_dir, ".env"))

# ===== INTERVIEW QUESTION GENERATOR =====

def generate_interview_questions(cv_text: str, jd_text: str, num_questions: int = 5) -> List[Dict]:
    """
    T·∫°o c√¢u h·ªèi ph·ªèng v·∫•n d·ª±a tr√™n CV v√† JD.
    
    Returns:
        List of questions with metadata:
        [
            {
                "id": 1,
                "question": "...",
                "type": "technical" | "behavioral" | "situational",
                "difficulty": "easy" | "medium" | "hard",
                "expected_keywords": ["keyword1", "keyword2"],
                "time_limit": 120  # seconds
            }
        ]
    """
    llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
    
    prompt = f"""B·∫°n l√† chuy√™n gia ph·ªèng v·∫•n tuy·ªÉn d·ª•ng. D·ª±a tr√™n CV v√† JD d∆∞·ªõi ƒë√¢y, h√£y t·∫°o {num_questions} c√¢u h·ªèi ph·ªèng v·∫•n.

CV:
{cv_text[:3000]}

JD:
{jd_text[:2000]}

Y√™u c·∫ßu:
- T·∫°o mix c√¢u h·ªèi: Technical (60%), Behavioral (30%), Situational (10%)
- ƒê·ªô kh√≥: 2 Easy, 2 Medium, 1 Hard
- M·ªói c√¢u h·ªèi ph·∫£i relevant v·ªõi CV v√† JD

Tr·∫£ v·ªÅ JSON array v·ªõi format:
[
    {{
        "id": 1,
        "question": "C√¢u h·ªèi ti·∫øng Vi·ªát",
        "question_en": "English version for TTS",
        "type": "technical",
        "difficulty": "easy",
        "expected_keywords": ["keyword1", "keyword2", "keyword3"],
        "ideal_answer_points": ["ƒêi·ªÉm 1 c·∫ßn ƒë·ªÅ c·∫≠p", "ƒêi·ªÉm 2 c·∫ßn ƒë·ªÅ c·∫≠p"],
        "time_limit": 120
    }}
]

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG TH√äM G√å KH√ÅC."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        # Clean JSON
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        
        questions = json.loads(content)
        return questions
    except Exception as e:
        print(f"Error generating questions: {e}")
        # Return default questions
        return [
            {
                "id": 1,
                "question": "H√£y gi·ªõi thi·ªáu v·ªÅ b·∫£n th√¢n b·∫°n.",
                "question_en": "Please introduce yourself.",
                "type": "behavioral",
                "difficulty": "easy",
                "expected_keywords": ["kinh nghi·ªám", "k·ªπ nƒÉng", "m·ª•c ti√™u"],
                "ideal_answer_points": ["T√™n v√† background", "Kinh nghi·ªám li√™n quan", "M·ª•c ti√™u ngh·ªÅ nghi·ªáp"],
                "time_limit": 120
            },
            {
                "id": 2,
                "question": "ƒêi·ªÉm m·∫°nh l·ªõn nh·∫•t c·ªßa b·∫°n l√† g√¨?",
                "question_en": "What is your greatest strength?",
                "type": "behavioral",
                "difficulty": "easy",
                "expected_keywords": ["k·ªπ nƒÉng", "th√†nh t√≠ch", "v√≠ d·ª•"],
                "ideal_answer_points": ["N√™u ƒëi·ªÉm m·∫°nh c·ª• th·ªÉ", "V√≠ d·ª• minh h·ªça", "Li√™n quan ƒë·∫øn c√¥ng vi·ªác"],
                "time_limit": 90
            }
        ]


# ===== VIDEO/AUDIO ANALYSIS =====

def analyze_video_frame(frame_base64: str) -> Dict:
    """
    Ph√¢n t√≠ch frame video ƒë·ªÉ ƒë√°nh gi√°:
    - Bi·ªÉu c·∫£m khu√¥n m·∫∑t
    - √Ånh m·∫Øt (eye contact)
    - T∆∞ th·∫ø
    - ƒê·ªô t·ª± tin
    """
    # Validate input
    if not frame_base64 or len(frame_base64) < 100:
        return {
            "facial_expression": {"score": 5, "note": "Frame kh√¥ng h·ª£p l·ªá"},
            "eye_contact": {"score": 5, "note": "Frame kh√¥ng h·ª£p l·ªá"},
            "posture": {"score": 5, "note": "Frame kh√¥ng h·ª£p l·ªá"},
            "confidence": {"score": 5, "note": "Frame kh√¥ng h·ª£p l·ªá"},
            "overall_note": "Frame kh√¥ng h·ª£p l·ªá"
        }
    
    try:
        llm = ChatOpenAI(model="gpt-4o", temperature=0)
        
        prompt = """Ph√¢n t√≠ch h√¨nh ·∫£nh ng∆∞·ªùi ƒëang ph·ªèng v·∫•n. ƒê√°nh gi√° ƒëi·ªÉm t·ª´ 1-10:

1. facial_expression (bi·ªÉu c·∫£m): t·ª± tin hay cƒÉng th·∫≥ng?
2. eye_contact (√°nh m·∫Øt): nh√¨n camera hay nh√¨n ch·ªó kh√°c?
3. posture (t∆∞ th·∫ø): ng·ªìi th·∫≥ng, chuy√™n nghi·ªáp?
4. confidence (t·ª± tin): t·ªïng th·ªÉ t·ª± tin?

Tr·∫£ v·ªÅ ƒê√öNG format JSON sau (kh√¥ng th√™m g√¨ kh√°c):
{"facial_expression":{"score":7,"note":"nhan xet"},"eye_contact":{"score":7,"note":"nhan xet"},"posture":{"score":7,"note":"nhan xet"},"confidence":{"score":7,"note":"nhan xet"},"overall_note":"nhan xet chung"}"""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame_base64}"}}
            ]
        )
        
        response = llm.invoke([message])
        content = response.content.strip()
        
        # Clean JSON
        if "```" in content:
            # Extract content between ```
            parts = content.split("```")
            for part in parts:
                part = part.strip()
                if part.startswith("json"):
                    part = part[4:].strip()
                if part.startswith("{"):
                    content = part
                    break
        
        # Find JSON object
        start = content.find("{")
        end = content.rfind("}") + 1
        if start >= 0 and end > start:
            content = content[start:end]
        
        return json.loads(content)
    except json.JSONDecodeError as e:
        print(f"Video analysis JSON error: {e}")
        # Return default scores
        return {
            "facial_expression": {"score": 6, "note": "B√¨nh th∆∞·ªùng"},
            "eye_contact": {"score": 6, "note": "B√¨nh th∆∞·ªùng"},
            "posture": {"score": 6, "note": "B√¨nh th∆∞·ªùng"},
            "confidence": {"score": 6, "note": "B√¨nh th∆∞·ªùng"},
            "overall_note": "ƒê√°nh gi√° m·∫∑c ƒë·ªãnh"
        }
    except Exception as e:
        print(f"Video analysis error: {e}")
        return {
            "facial_expression": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "eye_contact": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "posture": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "confidence": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "overall_note": f"L·ªói: {str(e)[:50]}"
        }


def transcribe_audio(audio_base64: str, audio_format: str = "webm") -> str:
    """
    Chuy·ªÉn audio th√†nh text s·ª≠ d·ª•ng OpenAI Whisper API.
    H·ªó tr·ª£: flac, m4a, mp3, mp4, mpeg, mpga, oga, ogg, wav, webm
    """
    print(f"\n=== TRANSCRIBE AUDIO ===")
    print(f"Audio base64 length: {len(audio_base64) if audio_base64 else 0}")
    
    if not audio_base64 or len(audio_base64) < 1000:
        print("Audio data too short or empty")
        return ""
    
    try:
        import openai
        client = openai.OpenAI()
        
        # Decode base64
        try:
            audio_bytes = base64.b64decode(audio_base64)
            print(f"Decoded audio bytes: {len(audio_bytes)}")
        except Exception as e:
            print(f"Base64 decode error: {e}")
            return ""
        
        if len(audio_bytes) < 1000:
            print(f"Audio file too small: {len(audio_bytes)} bytes")
            return ""
        
        # Try webm format (most common from browser)
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
                tmp.write(audio_bytes)
                tmp_path = tmp.name
            
            print(f"Saved temp file: {tmp_path}")
            
            # Transcribe with Whisper
            with open(tmp_path, "rb") as audio_file:
                print("Calling Whisper API...")
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="vi"
                )
            
            print(f"Whisper result: {transcript.text[:100] if transcript.text else 'EMPTY'}...")
            
            # Cleanup
            if tmp_path:
                try:
                    os.unlink(tmp_path)
                except:
                    pass
            
            return transcript.text if transcript.text else ""
                
        except Exception as e:
            print(f"Whisper transcription error: {e}")
            # Cleanup
            if tmp_path:
                try:
                    os.unlink(tmp_path)
                except:
                    pass
            return ""
        
    except Exception as e:
        print(f"Transcription error: {e}")
        import traceback
        traceback.print_exc()
        return ""


def analyze_voice_quality(audio_base64: str) -> Dict:
    """
    Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng gi·ªçng n√≥i.
    S·ª≠ d·ª•ng GPT ƒë·ªÉ ƒë√°nh gi√° t·ª´ transcript.
    """
    # First transcribe
    transcript = transcribe_audio(audio_base64)
    
    if not transcript:
        print("No transcript available, using default voice analysis")
        return {
            "clarity": {"score": 6, "note": "Kh√¥ng th·ªÉ ph√¢n t√≠ch audio"},
            "pace": {"score": 6, "note": "Kh√¥ng th·ªÉ ph√¢n t√≠ch audio"},
            "filler_words": {"score": 6, "note": "Kh√¥ng th·ªÉ ph√¢n t√≠ch audio"},
            "content_quality": {"score": 6, "note": "Kh√¥ng th·ªÉ ph√¢n t√≠ch audio"},
            "transcript": "[Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi audio th√†nh text]"
        }
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    prompt = f"""Ph√¢n t√≠ch transcript sau t·ª´ m·ªôt bu·ªïi ph·ªèng v·∫•n:

TRANSCRIPT:
"{transcript}"

ƒê√°nh gi√°:
1. **ƒê·ªô r√µ r√†ng** (1-10): C√¢u tr·∫£ l·ªùi c√≥ m·∫°ch l·∫°c kh√¥ng?
2. **T·ªëc ƒë·ªô n√≥i** (1-10): Qu√° nhanh, qu√° ch·∫≠m, hay v·ª´a ph·∫£i?
3. **T·ª´ ƒë·ªám** (1-10): C√≥ nhi·ªÅu "·ªù", "√†", "·ª´m" kh√¥ng? (10 = √≠t t·ª´ ƒë·ªám)
4. **Ch·∫•t l∆∞·ª£ng n·ªôi dung** (1-10): Tr·∫£ l·ªùi c√≥ ƒë√∫ng tr·ªçng t√¢m kh√¥ng?

Tr·∫£ v·ªÅ JSON:
{{
    "clarity": {{"score": 8, "note": "M·∫°ch l·∫°c, r√µ r√†ng"}},
    "pace": {{"score": 7, "note": "T·ªëc ƒë·ªô v·ª´a ph·∫£i"}},
    "filler_words": {{"score": 6, "note": "C√≥ m·ªôt s·ªë t·ª´ ƒë·ªám"}},
    "content_quality": {{"score": 7, "note": "Tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m"}},
    "transcript": "{transcript[:500]}"
}}

CH·ªà TR·∫¢ V·ªÄ JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        
        result = json.loads(content)
        result["transcript"] = transcript
        return result
    except Exception as e:
        print(f"Voice analysis error: {e}")
        return {
            "clarity": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "pace": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "filler_words": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "content_quality": {"score": 5, "note": "L·ªói ph√¢n t√≠ch"},
            "transcript": transcript
        }


# ===== ANSWER EVALUATION =====

def evaluate_answer(
    question: Dict,
    transcript: str,
    cv_text: str = "",
    jd_text: str = ""
) -> Dict:
    """
    ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi ph·ªèng v·∫•n.
    
    Returns:
        {
            "relevance_score": 8,  # ƒê·ªô li√™n quan v·ªõi c√¢u h·ªèi
            "completeness_score": 7,  # ƒê·ªô ƒë·∫ßy ƒë·ªß
            "accuracy_score": 8,  # ƒê·ªô ch√≠nh x√°c (n·∫øu l√† technical)
            "keywords_found": ["keyword1", "keyword2"],
            "keywords_missing": ["keyword3"],
            "strengths": ["ƒêi·ªÉm m·∫°nh 1", "ƒêi·ªÉm m·∫°nh 2"],
            "improvements": ["C·∫ßn c·∫£i thi·ªán 1"],
            "ideal_answer": "C√¢u tr·∫£ l·ªùi m·∫´u",
            "overall_score": 7.5,
            "feedback": "Nh·∫≠n x√©t chi ti·∫øt"
        }
    """
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    expected_keywords = question.get("expected_keywords", [])
    ideal_points = question.get("ideal_answer_points", [])
    question_type = question.get("type", "behavioral")
    
    prompt = f"""B·∫°n l√† chuy√™n gia ph·ªèng v·∫•n. ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi sau:

C√ÇU H·ªéI: {question.get('question', '')}
LO·∫†I: {question_type}
C√ÅC ƒêI·ªÇM C·∫¶N ƒê·ªÄ C·∫¨P: {', '.join(ideal_points)}
T·ª™ KH√ìA MONG ƒê·ª¢I: {', '.join(expected_keywords)}

C√ÇU TR·∫¢ L·ªúI C·ª¶A ·ª®NG VI√äN:
"{transcript}"

{f"TH√îNG TIN CV: {cv_text[:1000]}" if cv_text else ""}
{f"Y√äU C·∫¶U JD: {jd_text[:1000]}" if jd_text else ""}

H√£y ƒë√°nh gi√° v√† tr·∫£ v·ªÅ JSON:
{{
    "relevance_score": 8,
    "completeness_score": 7,
    "accuracy_score": 8,
    "keywords_found": ["t·ª´ kh√≥a t√¨m th·∫•y"],
    "keywords_missing": ["t·ª´ kh√≥a thi·∫øu"],
    "strengths": ["ƒêi·ªÉm m·∫°nh c·ªßa c√¢u tr·∫£ l·ªùi"],
    "improvements": ["C·∫ßn c·∫£i thi·ªán"],
    "ideal_answer": "C√¢u tr·∫£ l·ªùi m·∫´u ng·∫Øn g·ªçn",
    "overall_score": 7.5,
    "feedback": "Nh·∫≠n x√©t chi ti·∫øt 2-3 c√¢u"
}}

CH·ªà TR·∫¢ V·ªÄ JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        
        return json.loads(content)
    except Exception as e:
        print(f"Answer evaluation error: {e}")
        return {
            "relevance_score": 5,
            "completeness_score": 5,
            "accuracy_score": 5,
            "keywords_found": [],
            "keywords_missing": expected_keywords,
            "strengths": [],
            "improvements": ["Kh√¥ng th·ªÉ ƒë√°nh gi√°"],
            "ideal_answer": "",
            "overall_score": 5,
            "feedback": f"L·ªói ƒë√°nh gi√°: {str(e)}"
        }


# ===== BEHAVIORAL ASSESSMENT =====

def assess_behavioral(
    video_analyses: List[Dict],
    voice_analyses: List[Dict],
    answer_evaluations: List[Dict]
) -> Dict:
    """
    ƒê√°nh gi√° t·ªïng th·ªÉ behavioral/soft skills c·ªßa ·ª©ng vi√™n.
    
    Returns:
        {
            "communication_score": 8,
            "confidence_score": 7,
            "professionalism_score": 8,
            "body_language_score": 7,
            "overall_behavioral_score": 7.5,
            "strengths": ["ƒêi·ªÉm m·∫°nh"],
            "areas_to_improve": ["C·∫ßn c·∫£i thi·ªán"],
            "hiring_recommendation": "Recommend" | "Consider" | "Not Recommend",
            "detailed_feedback": "Nh·∫≠n x√©t chi ti·∫øt"
        }
    """
    llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
    
    # Aggregate scores
    video_summary = []
    for v in video_analyses:
        video_summary.append({
            "confidence": v.get("confidence", {}).get("score", 5),
            "eye_contact": v.get("eye_contact", {}).get("score", 5),
            "expression": v.get("facial_expression", {}).get("score", 5)
        })
    
    voice_summary = []
    for v in voice_analyses:
        voice_summary.append({
            "clarity": v.get("clarity", {}).get("score", 5),
            "pace": v.get("pace", {}).get("score", 5),
            "content": v.get("content_quality", {}).get("score", 5)
        })
    
    answer_summary = []
    for a in answer_evaluations:
        answer_summary.append({
            "relevance": a.get("relevance_score", 5),
            "completeness": a.get("completeness_score", 5),
            "overall": a.get("overall_score", 5)
        })
    
    prompt = f"""B·∫°n l√† chuy√™n gia ƒë√°nh gi√° ·ª©ng vi√™n. D·ª±a tr√™n d·ªØ li·ªáu ph·ªèng v·∫•n sau, ƒë√°nh gi√° t·ªïng th·ªÉ:

VIDEO ANALYSIS (bi·ªÉu c·∫£m, eye contact, t·ª± tin):
{json.dumps(video_summary, indent=2)}

VOICE ANALYSIS (r√µ r√†ng, t·ªëc ƒë·ªô, n·ªôi dung):
{json.dumps(voice_summary, indent=2)}

ANSWER QUALITY (li√™n quan, ƒë·∫ßy ƒë·ªß, overall):
{json.dumps(answer_summary, indent=2)}

ƒê√°nh gi√° v√† tr·∫£ v·ªÅ JSON:
{{
    "communication_score": 8,
    "confidence_score": 7,
    "professionalism_score": 8,
    "body_language_score": 7,
    "overall_behavioral_score": 7.5,
    "strengths": ["ƒêi·ªÉm m·∫°nh 1", "ƒêi·ªÉm m·∫°nh 2"],
    "areas_to_improve": ["C·∫ßn c·∫£i thi·ªán 1", "C·∫ßn c·∫£i thi·ªán 2"],
    "hiring_recommendation": "Recommend",
    "detailed_feedback": "Nh·∫≠n x√©t chi ti·∫øt 3-5 c√¢u v·ªÅ ·ª©ng vi√™n"
}}

hiring_recommendation: "Recommend" (>=7.5), "Consider" (5-7.5), "Not Recommend" (<5)

CH·ªà TR·∫¢ V·ªÄ JSON."""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        content = response.content.strip()
        
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        
        return json.loads(content)
    except Exception as e:
        print(f"Behavioral assessment error: {e}")
        return {
            "communication_score": 5,
            "confidence_score": 5,
            "professionalism_score": 5,
            "body_language_score": 5,
            "overall_behavioral_score": 5,
            "strengths": [],
            "areas_to_improve": ["Kh√¥ng th·ªÉ ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß"],
            "hiring_recommendation": "Consider",
            "detailed_feedback": f"L·ªói ƒë√°nh gi√°: {str(e)}"
        }


# ===== FULL INTERVIEW SESSION =====

class InterviewSession:
    """
    Qu·∫£n l√Ω m·ªôt phi√™n ph·ªèng v·∫•n ho√†n ch·ªânh.
    """
    
    def __init__(self, cv_text: str = "", jd_text: str = ""):
        self.cv_text = cv_text
        self.jd_text = jd_text
        self.questions = []
        self.current_question_idx = 0
        self.answers = []
        self.video_analyses = []
        self.voice_analyses = []
        self.answer_evaluations = []
        self.started_at = None
        self.ended_at = None
    
    def start_interview(self, num_questions: int = 5) -> List[Dict]:
        """B·∫Øt ƒë·∫ßu ph·ªèng v·∫•n, t·∫°o c√¢u h·ªèi."""
        self.questions = generate_interview_questions(
            self.cv_text, 
            self.jd_text, 
            num_questions
        )
        self.started_at = datetime.now().isoformat()
        self.current_question_idx = 0
        return self.questions
    
    def get_current_question(self) -> Optional[Dict]:
        """L·∫•y c√¢u h·ªèi hi·ªán t·∫°i."""
        if self.current_question_idx < len(self.questions):
            return self.questions[self.current_question_idx]
        return None
    
    def submit_answer(
        self,
        video_frames: List[str] = None,
        audio_base64: str = None,
        text_answer: str = None  # Direct text answer (fallback)
    ) -> Dict:
        """
        Submit c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi hi·ªán t·∫°i.
        H·ªó tr·ª£: video + audio ho·∫∑c text tr·ª±c ti·∫øp.
        
        Returns evaluation result.
        """
        current_q = self.get_current_question()
        if not current_q:
            return {"error": "No more questions"}
        
        result = {
            "question_id": current_q["id"],
            "question": current_q["question"]
        }
        
        # Analyze video frames (only if provided and valid)
        if video_frames and len(video_frames) > 0:
            try:
                # Only analyze first frame to save time
                if video_frames[0] and len(video_frames[0]) > 100:
                    analysis = analyze_video_frame(video_frames[0])
                    self.video_analyses.append(analysis)
                    result["video_analysis"] = analysis
            except Exception as e:
                print(f"Video analysis skipped: {e}")
        
        # Determine transcript source
        transcript = ""
        
        print(f"\n--- Processing Answer ---")
        print(f"text_answer received: '{text_answer}'")
        print(f"text_answer type: {type(text_answer)}")
        print(f"audio_base64 length: {len(audio_base64) if audio_base64 else 0}")
        
        # Priority 1: Direct text answer (most reliable)
        if text_answer is not None and str(text_answer).strip():
            transcript = str(text_answer).strip()
            result["input_mode"] = "text"
            print(f"Using TEXT mode, transcript: '{transcript[:100]}...'")
        
        # Priority 2: Audio transcription
        elif audio_base64 and len(str(audio_base64)) > 1000:
            try:
                print("Using AUDIO mode...")
                voice_analysis = analyze_voice_quality(audio_base64)
                self.voice_analyses.append(voice_analysis)
                transcript = voice_analysis.get("transcript", "")
                result["voice_analysis"] = voice_analysis
                result["input_mode"] = "audio"
                print(f"Audio transcript: '{transcript[:100] if transcript else 'EMPTY'}...'")
            except Exception as e:
                print(f"Voice analysis skipped: {e}")
        else:
            print("NO INPUT RECEIVED - text_answer is empty/None and no audio")
        
        # Evaluate answer if we have transcript
        if transcript:
            answer_eval = evaluate_answer(
                current_q,
                transcript,
                self.cv_text,
                self.jd_text
            )
            self.answer_evaluations.append(answer_eval)
            result["answer_evaluation"] = answer_eval
            result["transcript"] = transcript
        else:
            # No transcript - give default evaluation
            result["answer_evaluation"] = {
                "relevance_score": 5,
                "completeness_score": 5,
                "overall_score": 5,
                "feedback": "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i."
            }
        
        # Store answer
        self.answers.append({
            "question_id": current_q["id"],
            "transcript": transcript,
            "result": result
        })
        
        # Move to next question
        self.current_question_idx += 1
        
        return result
    
    def finish_interview(self) -> Dict:
        """
        K·∫øt th√∫c ph·ªèng v·∫•n v√† t·∫°o b√°o c√°o t·ªïng h·ª£p.
        """
        self.ended_at = datetime.now().isoformat()
        
        # Behavioral assessment
        behavioral = assess_behavioral(
            self.video_analyses,
            self.voice_analyses,
            self.answer_evaluations
        )
        
        # Calculate overall scores
        avg_answer_score = sum(
            a.get("overall_score", 5) for a in self.answer_evaluations
        ) / max(len(self.answer_evaluations), 1)
        
        report = {
            "session_info": {
                "started_at": self.started_at,
                "ended_at": self.ended_at,
                "total_questions": len(self.questions),
                "questions_answered": len(self.answers)
            },
            "scores": {
                "average_answer_score": round(avg_answer_score, 2),
                "behavioral_score": behavioral.get("overall_behavioral_score", 5),
                "communication_score": behavioral.get("communication_score", 5),
                "confidence_score": behavioral.get("confidence_score", 5)
            },
            "behavioral_assessment": behavioral,
            "question_results": self.answers,
            "recommendation": behavioral.get("hiring_recommendation", "Consider"),
            "summary": self._generate_summary(behavioral, avg_answer_score)
        }
        
        return report
    
    def _generate_summary(self, behavioral: Dict, avg_score: float) -> str:
        """T·∫°o summary text."""
        rec = behavioral.get("hiring_recommendation", "Consider")
        
        if rec == "Recommend":
            status = "‚úÖ ƒê·ªÄ XU·∫§T TUY·ªÇN D·ª§NG"
        elif rec == "Consider":
            status = "üü° C·∫¶N C√ÇN NH·∫ÆC TH√äM"
        else:
            status = "‚ùå CH∆ØA PH√ô H·ª¢P"
        
        return f"""
## üìä K·∫æT QU·∫¢ PH·ªéNG V·∫§N

### {status}

**ƒêi·ªÉm trung b√¨nh c√¢u tr·∫£ l·ªùi:** {avg_score:.1f}/10
**ƒêi·ªÉm behavioral:** {behavioral.get('overall_behavioral_score', 5):.1f}/10

### üí™ ƒêi·ªÉm m·∫°nh:
{chr(10).join('- ' + s for s in behavioral.get('strengths', []))}

### üìà C·∫ßn c·∫£i thi·ªán:
{chr(10).join('- ' + s for s in behavioral.get('areas_to_improve', []))}

### üí¨ Nh·∫≠n x√©t:
{behavioral.get('detailed_feedback', '')}
"""


# ===== EXPORT FUNCTIONS =====

def create_interview_session(cv_text: str = "", jd_text: str = "") -> InterviewSession:
    """Factory function to create interview session."""
    return InterviewSession(cv_text, jd_text)


# For testing
if __name__ == "__main__":
    # Test question generation
    test_cv = "Python Developer v·ªõi 3 nƒÉm kinh nghi·ªám, bi·∫øt Django, Flask, Machine Learning"
    test_jd = "Tuy·ªÉn Python Developer, y√™u c·∫ßu 2+ nƒÉm kinh nghi·ªám, bi·∫øt Django"
    
    questions = generate_interview_questions(test_cv, test_jd, 3)
    print("Generated Questions:")
    for q in questions:
        print(f"  {q['id']}. {q['question']}")

